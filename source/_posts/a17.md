title: Mongodb CDCé“¾è·¯ä¸­çš„æœ‰åºæ€§
date: 2020-03-22
tags: [monogdb,kafka,flink]
toc: true
---

åœ¨æˆ‘ä»¬çš„ä¸šåŠ¡å½“ä¸­æœ‰è¿™æ ·ä¸€ä¸ªåœºæ™¯ï¼Œç”¨æˆ·æ¯æ”¶åˆ°ä¸€ç¬”è½¬è´¦ä»¥åï¼Œå°±ä¼šé€šè¿‡æ¨é€æœåŠ¡ç»™ç”¨æˆ·å‘é€ä¸€ä¸ªé€šçŸ¥ï¼Œè€Œè¿™äº›é€šçŸ¥ä¹‹é—´éœ€è¦ä¿è¯å…ˆåé¡ºåºï¼ˆå³å¸å•çš„äº§ç”Ÿæ—¶åºï¼‰ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè®¢å•æ•°æ®ç»“æ„`Bill`ï¼š
```json
{
	"_id": ObjectId
	"fromUserId": String,
	"toUserId": String,
	"amount": Double,
	"createdAt": Long
}
```
`fromUserId`æ˜¯è½¬è´¦ç”¨æˆ·çš„idï¼Œ`toUserId`æ˜¯æ”¶æ¬¾ç”¨æˆ·çš„idï¼Œ`amount`æ˜¯è½¬è´¦é‡‘é¢ï¼Œ`createdAt`æ˜¯è®°å½•åˆ›å»ºçš„æ—¥æœŸã€‚

ç³»ç»Ÿä¸­ä½¿ç”¨å®‰è£…äº†`debezium connector for mongodb`æ’ä»¶çš„`kafka connect`æ¥å°†mongodb oplogæ”¶é›†åˆ°kafkaä¸­ï¼Œä¸‹æ¸¸ä½¿ç”¨`flink streaming task`æ¥è¿›è¡Œæ¶ˆè´¹å¹¶è§¦å‘æ¨é€æœåŠ¡ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­æ•°æ®ä¼šæµè¿‡å¤šä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå¦‚ä½•ä¿è¯åœ¨æµç»è¿™äº›ç³»ç»Ÿä»¥åè¿˜èƒ½ä¿è¯è®°å½•çš„äº§ç”Ÿé¡ºåºå°±æ˜¯ä»Šå¤©è¦è®¨è®ºçš„é—®é¢˜ã€‚

## Kafka connect && Kafka
æ‰€æœ‰çš„`Bill oplog`éƒ½ä¼šè¢«å‘é€åˆ°åŒä¸€ä¸ª`Kafka topic`ä¸­ï¼Œæ‰€ä»¥åªéœ€è¦ä¿è¯åœ¨å‘å¾€Kafkaçš„è¿‡ç¨‹å½“ä¸­ä½¿ç”¨`toUserId`ä½œä¸º Partition Keyå³å¯ã€‚ä½†æ˜¯ç”±äºä½¿ç”¨äº†å¼€æºç»„ä»¶æ¥å¸®åŠ©æˆ‘ä»¬å®Œæˆäº†æ”¶é›†oplogçš„ä»»åŠ¡ï¼Œæ‰€ä»¥éœ€è¦ä¿è¯`debezium connector for mongodb`èƒ½å¦‚æˆ‘ä»¬çš„æ„¿ã€‚

é¦–å…ˆéœ€è¦çŸ¥é“çš„æ˜¯`Kafka connect`é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨çš„åˆ†åŒºç­–ç•¥æ˜¯`org.apache.kafka.clients.producer.DefaultPartitioner`ï¼Œå…¶ä¸­`partition`æ–¹æ³•çš„å®ç°å¦‚ä¸‹ï¼š
```java
/**
 * Compute the partition for the given record.
 *
 * @param topic The topic name
 * @param key The key to partition on (or null if no key)
 * @param keyBytes serialized key to partition on (or null if no key)
 * @param value The value to partition on or null
 * @param valueBytes serialized value to partition on or null
 * @param cluster The current cluster metadata
 */
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
    int numPartitions = partitions.size();
    if (keyBytes == null) {
        int nextValue = nextValue(topic);
        List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
        if (availablePartitions.size() > 0) {
            int part = Utils.toPositive(nextValue) % availablePartitions.size();
            return availablePartitions.get(part).partition();
        } else {
            // no partitions are available, give a non-available partition
            return Utils.toPositive(nextValue) % numPartitions;
        }
    } else {
        // hash the keyBytes to choose a partition
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }
}

private int nextValue(String topic) {
    AtomicInteger counter = topicCounterMap.get(topic);
    if (null == counter) {
        counter = new AtomicInteger(ThreadLocalRandom.current().nextInt());
        AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);
        if (currentCounter != null) {
            counter = currentCounter;
        }
    }
    return counter.getAndIncrement();
}
```
å¦‚æœkeyæ˜¯ç©ºçš„ï¼Œåˆ™ä»¥è½®è®­çš„æ–¹å¼æ¥åˆ†é…`partition`ï¼Œå¦åˆ™åˆ™ä½¿ç”¨ä¸€ç§`murmur2`çš„HASHç®—æ³•æ¥åˆ†é…`partition`ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°†message keyè®¾ç½®ä¸º`toUserId`å³å¯è¾¾æˆæˆ‘ä»¬çš„ç›®çš„ã€‚ä½†æ˜¯ [Debezium Connector for MongoDB :: Change eventâ€™s key](https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#mongodb-change-events-key) ä¸­å†™é“ï¼Œmessage keyåªèƒ½æ˜¯`_id`ï¼Œè€Œ`toUserId`ä¸å…·æœ‰å”¯ä¸€æ€§ï¼Œæ‰€ä»¥ä¸èƒ½ä½œä¸º`_id`ï¼›å¦‚æœåŠ ä¸Šæ—¶é—´æˆ³æˆ–è€…å…¶ä»–çš„å­—æ®µä¿è¯äº†å”¯ä¸€æ€§ï¼Œåˆå¤±å»äº†è¦å°†ç›¸åŒ`toUserId`æ”¾åœ¨ä¸€ä¸ªåˆ†åŒºçš„è¯­ä¹‰ï¼Œæ‰€ä»¥è¿™æ¡è·¯åŸºæœ¬æ˜¯èµ°ä¸é€šçš„ã€‚
```
The MongoDB connector does not make any explicit determination of the topic partitions for events. Instead, it allows Kafka to determine the partition based upon the key. You can change Kafkaâ€™s partitioning logic by defining in the Kafka Connect worker configuration the name of the Partitioner implementation.
Be aware that Kafka only maintains total order for events written to a single topic partition. Partitioning the events by key does mean that all events with the same key will always go to the same partition, ensuring that all events for a specific document are always totally ordered.
```
ç»§ç»­å¯»æ‰¾å…¶ä»–çš„å˜é€šæ–¹æ³•ï¼Œ[Debezium Connector for MongoDB :: Partitions](https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#partitions) æ–‡æ¡£ä¸­å†™åˆ°å¯ä»¥é€šè¿‡è®¾ç½® Kafka connect worker çš„`Partitioner`è®¾ç½®æ¥æŒ‡å®šåˆ†åŒºç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯ [Apache Kafka](http://kafka.apache.org/documentation.html#producerconfigs) æ–‡æ¡£ä¸­æåˆ°çš„`partitioner.class`ï¼š
```
partitioner.class: Partitioner class that implements the org.apache.kafka.clients.producer.Partitioner interface.
Type: classDefault: org.apache.kafka.clients.producer.internals.DefaultPartitioner
```
å³éœ€è¦è‡ªå·±é€šè¿‡ç»§æ‰¿`Partitioner`æ¥å£æ¥å®ç°è‡ªå·±çš„åˆ†åŒºç­–ç•¥ï¼Œè¦å®ç°`partitionæ–¹æ³•`ï¼š
```java
 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
	...
}
```
å®ç°äº†ä¹‹åå°†åŒ…å«è¯¥ç±»çš„JARåŒ…æ”¾å…¥Kafka connectèƒ½æ‰«æçš„è·¯å¾„ä¸‹ï¼Œå†åœ¨`connect-standalone.properties`æˆ–è€…`connect-distributed.properties`é…ç½®æ–‡ä»¶ä¸­è¿›è¡Œå…¨å±€çš„è®¾å®šã€‚å¯ä»¥å‚è§ä¸‹é¢ä¸¤ä¸ªé“¾æ¥ğŸ‘‡ï¼š
+ [java - Custom partition assignment in Kafka JDBC connector - Stack Overflow](https://stackoverflow.com/questions/55188508/custom-partition-assignment-in-kafka-jdbc-connector)
+ [java - Setting Partition Strategy in a Kafka Connector - Stack Overflow](https://stackoverflow.com/questions/44810221/setting-partition-strategy-in-a-kafka-connector)

é‚£ä¹ˆè¯•æƒ³ä¸€ä¸‹æˆ‘ä»¬çš„æ–¹æ¡ˆæ˜¯ä¸æ˜¯å°±å¯ä»¥å®ç°ä¸ºï¼Œå°†`toUserId`å’Œæ—¶é—´æˆ³æ‹¼æ¥ä¸ºå”¯ä¸€æ€§çš„`_id`ï¼Œåœ¨partitionå‡½æ•°ä¸­å°†`toUserId`æå–å‡ºæ¥å¹¶ä»¥æ­¤ä½œä¸ºpartition keyè¿›è¡Œåˆ†åŒºä»¥å®ç°åœ¨kafkaä¸­ä¿è¯é¡ºåºçš„ç›®çš„ã€‚ä½†æ˜¯è¿™æ ·çš„å®ç°æ–¹æ¡ˆä¹Ÿå­˜åœ¨é—®é¢˜ï¼Œæ¯”å¦‚è¿™ä¸ªKafka connect ä»…æ¥ä¸ºè¿™ä¸ªä»»åŠ¡æœåŠ¡ï¼Œå› ä¸ºè¿™ç§åˆ†åŒºç­–ç•¥å¹¶ä¸å…·æœ‰é€šç”¨æ€§ï¼Œå¦‚æœæœ‰å¤šä¸ªç±»ä¼¼çš„éœ€æ±‚åˆ™è¦éƒ¨ç½²å¤šä»½Kafka connectã€‚åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€äº›å¥½å¤„ï¼Œä¾‹å¦‚æ‹¼æ¥çš„`_id`æ˜¯å¯ä»¥ç›´æ¥ä½œä¸ºMongodbçš„shard keyæ¥å¯¹è¯¥åœºæ™¯çš„ä¸Šæ¸¸è¿›è¡Œæ¨ªå‘æ‰©å±•çš„ã€‚å¯æƒœçš„æ˜¯debeziumçš„å®ç°å¹¶ä¸èƒ½ä¿è¯shard clusterä¼ å…¥kafkaæ—¶ä¿è¯äº‹ä»¶çš„å‘ç”Ÿé¡ºåºï¼Œè™½ç„¶èƒ½å°†æ‹¼æ¥çš„`_id`ä½œä¸ºshard keyï¼Œä½†æ˜¯ç”±äºè¿™ç§æ¶æ„å¹¶æ²¡æœ‰èƒ½åŠ›ä¿è¯é¡ºåºæ€§ï¼Œæ‰€ä»¥è¿™ç§æ‰©å±•ä¹Ÿæ˜¯æ— æ•ˆçš„ï¼Œå‚è§ğŸ‘‡ï¼š
[Debezium Connector for MongoDB :: MongoDB sharded cluster](https://debezium.io/documentation/reference/1.0/connectors/mongodb.html#mongodb-sharded-cluster)

ç»¼ä¸Šåœ¨è¯¥æ¶æ„ä¸­å¦‚æœæƒ³åœ¨Kafkaå±‚ä¿è¯äº‹ä»¶çš„æœ‰åºæ€§æ˜¯éå¸¸å›°éš¾ï¼Œå¹¶ä¸”å¾ˆä¸ç»æµå®æƒ ã€‚

## Flink streaming
é€šè¿‡ä¸Šæ–‡çš„åˆ†æï¼Œæˆ‘ä»¬ä¸å¾—ä¸æ¥å—ä¸€ä¸ªäº‹å®ï¼Œ`Flink`æ¶ˆè´¹åˆ°çš„æ˜¯ä¹±åºæ•°æ®ã€‚ä½†æ˜¯å› ä¸ºå…¶ç‰¹æ€§ï¼Œèƒ½è¾ƒä¸ºæ–¹ä¾¿çš„å¯¹ä¹±åºæ•°æ®è¿›è¡Œå¤„ç†ã€‚
åœ¨debeziumæ”¶é›†åˆ°çš„oplogä¸­ï¼ŒåŒ…å«ä¸¤ä¸ªæ—¶é—´æˆ³ï¼Œä¸€ä¸ªæ˜¯åœ¨Value payloadä¸­çš„`ts_ms`ï¼Œè¡¨ç¤ºçš„æ˜¯debeziumæ”¶é›†è¯¥oplogæ—¶çš„ç³»ç»Ÿæ—¶é—´ï¼›å¦å¤–ä¸€ä¸ªæ˜¯åœ¨Value payloadä¸­çš„`source.ts_ms`ï¼Œè¡¨ç¤ºçš„æ˜¯mongodbäº§ç”Ÿoplogçš„æ—¶é—´ã€‚è€Œæˆ‘ä»¬çš„å®ä½“ç±»ä¸­ä¹Ÿå†™å…¥äº†Create billçš„æ—¶é—´`createdAt`ï¼Œç”±äºåªå…³å¿ƒ`Insert`äº‹ä»¶ï¼Œæ‰€ä»¥åœ¨Value payloadçš„`after.createdAt`ä¸­èƒ½è·å–Billåˆ›å»ºçš„çœŸå®æ—¶é—´ã€‚åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ä¸€èˆ¬ä»¥ç¬¬äºŒä¸ªæ—¶é—´æˆ–è€…ç¬¬ä¸‰ä¸ªæ—¶é—´ä¸ºå‡†ï¼Œåœ¨æ²¡æœ‰é‡‡ç”¨shard clusterçš„mongodbä¸­ï¼Œæˆ‘è®¤ä¸ºç¬¬äºŒä¸ªæ—¶é—´çš„å…ˆåæ¬¡åºåº”è¯¥ä¸ç¬¬ä¸‰ä¸ªæ—¶é—´çš„å…ˆåæ¬¡åºç›¸åŒï¼›é‡‡ç”¨äº†shard clusterçš„mongodbä¸­ï¼Œåº”è¯¥ä»¥ç¬¬ä¸‰ä¸ªæ—¶é—´ä¸ºå‡†ã€‚
```json
"payload": {
      "after": ...,
      "patch": null,
      "source": {
        "version": "1.0.3.Final",
        "connector": "mongodb",
        "name": "cdc_test",
        "ts_ms": 1558965508000,
        "snapshot": true,
        "db": "inventory",
        "rs": "rs0",
        "collection": "bill",
        "ord": 31,
        "h": 1546547425148721999
      },
      "op": "r",
      "ts_ms": 1558965515240
    }
```

åœ¨Flinkçš„æ—¶é—´æ¦‚å¿µä¸­ä¹Ÿæœ‰ä¸‰ç§æ—¶é—´ï¼š
- äº‹ä»¶æ—¶é—´ï¼šç‹¬ç«‹äº‹ä»¶åœ¨äº§ç”Ÿå®ƒçš„è®¾å¤‡ä¸Šå‘ç”Ÿçš„äº‹ä»¶ï¼Œé€šå¸¸åœ¨è¿›å…¥Flinkä¹‹å‰å°±å·²ç»åµŒå…¥åˆ°äº‹ä»¶ä¸­ã€‚
- æ¥å…¥æ—¶é—´ï¼šæ•°æ®è¿›å…¥Flinkçš„æ—¶é—´ï¼Œå–å†³äºSource Operatoræ‰€åœ¨ä¸»æœºçš„ç³»ç»Ÿæ—¶é’Ÿã€‚
- å¤„ç†æ—¶é—´ï¼šæ•°æ®åœ¨æ“ä½œç®—å­è®¡ç®—è¿‡ç¨‹ä¸­è·å–åˆ°çš„æ‰€åœ¨ä¸»æœºæ—¶é—´ã€‚

æ˜¾ç„¶åœ¨è¯¥åœºæ™¯ä¸‹åº”è¯¥é€‰æ‹©`Bill`çš„åˆ›å»ºæ—¶é—´ä½œä¸ºFlinkçš„äº‹ä»¶æ—¶é—´ï¼š
```scala
env.addSource(Utils.providesAvroKafkaConsumer(kafkaConfig))
    .map(recordTuple => AvroMongoOplog.newInstance(recordTuple._1.asInstanceOf[Record], recordTuple._2.asInstanceOf[Record]))
    .filter(mongoOplog => mongoOplog.getOpType.eq(MongoOpType.Insert))
    .assignAscendingTimestamps(mongoOplog => mongoOplog.getDocument.getLong("createdAt"))
	  .keyBy(mongoOplog => mongoOplog.getDocument.get("toUserId"))
    .window(TumblingProcessingTimeWindows.of(Time.seconds(10)))
    .process(...)
```

+ è¿æ¥Kafkaï¼Œä½¿ç”¨å®ç°çš„`KeyedAvroDeserializationSchema`è¿›è¡Œååºåˆ—åŒ–
```java
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient
import org.apache.avro.generic.GenericRecord
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema
import org.apache.kafka.clients.consumer.ConsumerRecord

@SerialVersionUID(1584533572114L)
class KeyedAvroDeserializationSchema extends KafkaDeserializationSchema[(GenericRecord, GenericRecord)] {

    var keyDeserializer: ConfluentRegistryAvroDeserializationSchema[GenericRecord] = _
    var valueDeserializer: ConfluentRegistryAvroDeserializationSchema[GenericRecord] = _

    def this(topic: String, schemaRegistry: String) {
        this()
        val keySubject = topic + "-key"
        val valueSubject = topic + "-value"
        val schemaRegistryClient = new CachedSchemaRegistryClient(schemaRegistry, 1000)
        val keySchema = schemaRegistryClient.getByID(schemaRegistryClient.getLatestSchemaMetadata(keySubject).getId)
        val valueSchema = schemaRegistryClient.getByID(schemaRegistryClient.getLatestSchemaMetadata(valueSubject).getId)
        keyDeserializer = ConfluentRegistryAvroDeserializationSchema.forGeneric(keySchema, schemaRegistry)
        valueDeserializer = ConfluentRegistryAvroDeserializationSchema.forGeneric(valueSchema, schemaRegistry)
    }

    override def isEndOfStream(t: (GenericRecord, GenericRecord)): Boolean = false

    override def deserialize(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]]): (GenericRecord, GenericRecord)
    = (keyDeserializer.deserialize(consumerRecord.key()), valueDeserializer.deserialize(consumerRecord.value()))

    override def getProducedType: TypeInformation[(GenericRecord, GenericRecord)] = createTypeInformation[(GenericRecord, GenericRecord)]
}
```
+ é€šè¿‡ä¸€ä¸ªå·¥å…·ç±»å°†`(Record,Record)`è½¬åŒ–ä¸ºæ›´å¥½å¤„ç†çš„`MongoOplogEntry`ç±»å‹
+ è¿‡æ»¤å‡ºæ‰€æœ‰çš„`Insert`äº‹ä»¶
+ å°†`Bill`çš„åˆ›å»ºæ—¶é—´ä½œä¸ºFlinkçš„äº‹ä»¶æ—¶é—´
+ æŒ‰ç…§`toUserId`è¿›è¡Œåˆ†åŒº
+ è®¾ç½®10ç§’çš„æ—¶é—´çª—å£
+ åœ¨çª—å£å¤„ç†å‡½æ•°ä¸­å¯¹æ‰€æœ‰`MongoOplogEntry`å¯¹è±¡æŒ‰ç…§`createdAt`å†æ’åºåä¾æ¬¡è§¦å‘æ¨é€æœåŠ¡

## æœ€å
ä»¥ä¸Šä»‹ç»äº†åœ¨Mongodb CDCè¿‡ç¨‹ä¸­ä¿è¯æ•°æ®æœ‰åºæ€§çš„ä¸¤ç§æ€è·¯ï¼Œä¸€ç§æ˜¯åœ¨Kafkaä¸­å°±ä¿è¯`toUserId`ç›¸åŒçš„æ•°æ®å‡æœ‰åºï¼Œè¿™æ ·åœ¨æ¶ˆè´¹è¿‡ç¨‹ä¸­ä¸éœ€è¦åšçª—å£è®¡ç®—ï¼Œåªè¦åœ¨éœ€è¦partitionçš„åœ°æ–¹ç»§ç»­ä½¿ç”¨`toUserId`è¿›è¡Œpartitionï¼Œå°±èƒ½ä¿è¯æ•°æ®æœ‰åºæ€§ã€‚è¿™ç§æ–¹æ¡ˆåœ¨Kafka connectä¾§éœ€è¦åšå¾ˆå¤šçš„å·¥ä½œï¼Œä½†æ˜¯èƒ½ä¸ºæµå¼ä»»åŠ¡å¸¦æ¥æ›´å¥½çš„æ¶ˆè´¹æ€§èƒ½ï¼Œä½†æ˜¯ç”±äºdebeziumçš„å±€é™æ€§ï¼Œåœ¨shard clusterçš„Mongodbä¸­ä¸èƒ½å‘æŒ¥ä½œç”¨ã€‚ç¬¬äºŒç§æ˜¯è®©Flinkæ¶ˆè´¹ä¹±åºæ•°æ®ï¼Œä½¿ç”¨å…¶æœ¬èº«çš„äº‹ä»¶æ—¶é—´çª—å£è®¡ç®—æ¥é‡æ–°çº æ­£æ•°æ®ã€‚è¿™ç§æ–¹æ¡ˆä¼šè®©Flinkçš„æ•ˆç‡å¤§æ‰“æŠ˜æ‰£ï¼Œå¹¶ä¸”éœ€è¦ä¿è¯çª—å£ç¼“å­˜æ•°æ®ä¸èƒ½è¶…è¿‡é™åˆ¶ï¼Œä½†æ˜¯æ¯”è¾ƒé€šç”¨ï¼Œä½¿ç”¨çš„æ—¶å€™ä¹Ÿéœ€è¦æ³¨æ„è¿Ÿåˆ°çš„æ•°æ®è¯¥å¦‚ä½•å¤„ç†ã€‚å¤§å®¶å¯ä»¥æ ¹æ®è‡ªå·±çš„åœºæ™¯æ¥æŒ‘é€‰æ–¹æ¡ˆï¼Œæˆ–è€…å¦‚æœæœ‰æ›´å¥½çš„æ–¹æ¡ˆæ¬¢è¿ä¸€èµ·äº¤æµã€‚